{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G8TD17pT89K",
    "outputId": "ac9fa96e-0e70-46ee-995d-9d72d22fa193"
   },
   "outputs": [],
   "source": [
    "!mkdir wordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXQvXY7jUvMD",
    "outputId": "4fba4a49-f990-49c3-8e9d-3aea5ddad0b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MLqabAUgXPoF"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from filelock import FileLock\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers.utils import logging\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer #bertformaskedLM\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "logger = logging.get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a75xM9WsUwRi"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "HNMUWRoM5j6a",
    "outputId": "785fe9a6-11bd-4aa0-e78a-c10c9b58a30d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.20.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_review_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>í• ì¸ ë°›ì•„ì„œ ì¢‹ì€ ê°€ê²©ì— êµ¬ë§¤í–ˆì–´ìš” ë‚¨í¸ì´ ì´ê²ƒë§Œ ì‚¬ìš©í•´ì„œ ìì£¼êµ¬ë§¤ í•©ë‹ˆë‹¤ ì¢‹ì•„ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë³´í†µ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ì•„ì§ ì‚¬ìš©ì „ì´ê¸´ í•˜ë‚˜ ë§ ê·¸ëŒ€ë¡œ ìš”ê¸´í•˜ê²Œ ì“°ì¼ ë“¯í•˜ë„¤ìš” ì–´ë¬µê½‚ì´ í•  ë•Œ í•„ìš”í•´ì„œ ì£¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ë§Œì¡±í•©ë‹ˆë‹¤ ì „ ë¦¬í„° ì‹œì¼°ê³  ìš” ì†”ì°íˆ ë¹¨ë˜ê°€ ì € ê°™ì€ ê²½ìš° ë§ì„ ë•Œ ê°€ ì‡ ì˜… ì ì„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì¢‹ì•„ìš” íƒˆì°©í•  ìˆ˜ ìˆì–´ ë”ìš± ì¢‹ì•„ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042767</th>\n",
       "      <td>ì¼ êµ¬ì… í›„ ì°¨ë¡€ ê¸°ë¶„ ì¢‹ê²Œ ì‚¬ìš© í–ˆìŠµë‹ˆë‹¤ ì‚¬ìš© í›„ ì¡°ë¦¬ëŒ€ìœ„ì—ì„œ ëšœê»‘ ë¶„ë¦¬í•˜ì—¬ ì„¸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042768</th>\n",
       "      <td>ìˆ˜ê³ í•˜ì„¸ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042769</th>\n",
       "      <td>ì €í¬ ì§‘ì€ ì›ëª©ì‹íƒì´ë¼ ì•„ì´ë“¤ ì‹íƒ ë§¤íŠ¸ë¡œ ì‚¬ìš©í•˜ë ¤ê³  êµ¬ë§¤í–ˆì–´ìš” ì‚¬ì´ì¦ˆê°€ ì¸¡ì • í›„ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042770</th>\n",
       "      <td>ê°•ë¸”ë¦¬ ë¬¼ë§›ì´ ì¢‹ë„¤ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042771</th>\n",
       "      <td>ì§ë°°ì†¡ ë³µì‚¬ìš©ì§€ ìš©ì§€ ë³µì‚¬ì§€ ë§¤ ë°•ìŠ¤ ë”ë¸”ì— ì´</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042772 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pre_review_\n",
       "0           í• ì¸ ë°›ì•„ì„œ ì¢‹ì€ ê°€ê²©ì— êµ¬ë§¤í–ˆì–´ìš” ë‚¨í¸ì´ ì´ê²ƒë§Œ ì‚¬ìš©í•´ì„œ ìì£¼êµ¬ë§¤ í•©ë‹ˆë‹¤ ì¢‹ì•„ìš” \n",
       "1                                                       ë³´í†µ\n",
       "2        ì•„ì§ ì‚¬ìš©ì „ì´ê¸´ í•˜ë‚˜ ë§ ê·¸ëŒ€ë¡œ ìš”ê¸´í•˜ê²Œ ì“°ì¼ ë“¯í•˜ë„¤ìš” ì–´ë¬µê½‚ì´ í•  ë•Œ í•„ìš”í•´ì„œ ì£¼...\n",
       "3        ë§Œì¡±í•©ë‹ˆë‹¤ ì „ ë¦¬í„° ì‹œì¼°ê³  ìš” ì†”ì°íˆ ë¹¨ë˜ê°€ ì € ê°™ì€ ê²½ìš° ë§ì„ ë•Œ ê°€ ì‡ ì˜… ì ì„...\n",
       "4                                      ì¢‹ì•„ìš” íƒˆì°©í•  ìˆ˜ ìˆì–´ ë”ìš± ì¢‹ì•„ìš”\n",
       "...                                                    ...\n",
       "1042767   ì¼ êµ¬ì… í›„ ì°¨ë¡€ ê¸°ë¶„ ì¢‹ê²Œ ì‚¬ìš© í–ˆìŠµë‹ˆë‹¤ ì‚¬ìš© í›„ ì¡°ë¦¬ëŒ€ìœ„ì—ì„œ ëšœê»‘ ë¶„ë¦¬í•˜ì—¬ ì„¸...\n",
       "1042768                                              ìˆ˜ê³ í•˜ì„¸ìš”\n",
       "1042769  ì €í¬ ì§‘ì€ ì›ëª©ì‹íƒì´ë¼ ì•„ì´ë“¤ ì‹íƒ ë§¤íŠ¸ë¡œ ì‚¬ìš©í•˜ë ¤ê³  êµ¬ë§¤í–ˆì–´ìš” ì‚¬ì´ì¦ˆê°€ ì¸¡ì • í›„ ...\n",
       "1042770                                       ê°•ë¸”ë¦¬ ë¬¼ë§›ì´ ì¢‹ë„¤ìš” \n",
       "1042771                         ì§ë°°ì†¡ ë³µì‚¬ìš©ì§€ ìš©ì§€ ë³µì‚¬ì§€ ë§¤ ë°•ìŠ¤ ë”ë¸”ì— ì´\n",
       "\n",
       "[1042772 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_pickle('./Data/fur_train_data.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = data['pre_review_'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delab_7\\AppData\\Local\\Temp\\ipykernel_9368\\1359118793.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, line in tqdm_notebook(enumerate(doc[:-20000])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5644f62d532b4b03865a077ed3d98be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "with open('./Data/fur/final_fur_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, line in tqdm_notebook(enumerate(doc[:-20000])):\n",
    "#         for line in lines.split('. '):\n",
    "#             line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n",
    "#             line = re.sub(' +', ' ', line)\n",
    "        f.write(str(line) + '\\n')\n",
    "    if idx != len(doc[:-20000])-1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delab_7\\AppData\\Local\\Temp\\ipykernel_9368\\3341214965.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, line in tqdm_notebook(enumerate(doc[-20000:-10000])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faa61c159384fe88f31f3d5d434cf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validation\n",
    "with open('./Data/fur/final_fur_validation.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, line in tqdm_notebook(enumerate(doc[-20000:-10000])):\n",
    "#         for line in lines.split('. '):\n",
    "#             line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n",
    "#             line = re.sub(' +', ' ', line)\n",
    "        f.write(str(line) + '\\n')\n",
    "    if idx != len(doc[-20000:10000])-1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delab_7\\AppData\\Local\\Temp\\ipykernel_9368\\3402576644.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, line in tqdm_notebook(enumerate(doc[-10000:])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4a9fd8a1f54b6cba0a9263e2e619e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "with open('./Data/fur/final_fur_test.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, line in tqdm_notebook(enumerate(doc[-10000:])):\n",
    "#         for line in lines.split('. '):\n",
    "#             line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n",
    "#             line = re.sub(' +', ' ', line)\n",
    "        f.write(str(line) + '\\n')\n",
    "    if idx != len(doc[-10000:])-1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, BertForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108950064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YSmzT_Y8d_yE"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] ë¥¼ ì”Œìš°ëŠ” ê²ƒì€ ì €í¬ê°€ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids 0 = [PAD]\n",
      "Token ids 1 = [UNK]\n",
      "Token ids 2 = [CLS]\n",
      "Token ids 3 = [SEP]\n",
      "Token ids 4 = [MASK]\n"
     ]
    }
   ],
   "source": [
    "print(\"Token ids 0 = {}\".format(tokenizer.convert_ids_to_tokens(0)))\n",
    "print(\"Token ids 1 = {}\".format(tokenizer.convert_ids_to_tokens(1)))\n",
    "print(\"Token ids 2 = {}\".format(tokenizer.convert_ids_to_tokens(2)))\n",
    "print(\"Token ids 3 = {}\".format(tokenizer.convert_ids_to_tokens(3)))\n",
    "print(\"Token ids 4 = {}\".format(tokenizer.convert_ids_to_tokens(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at ./Data/fur/final_fur_train.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 76.78861427307129\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "dataset1 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./Data/fur/final_fur_train.txt',\n",
    "    block_size=300,\n",
    ")\n",
    "\n",
    "print(\"time :\", time.time() - start)  # í˜„ì¬ì‹œê° - ì‹œì‘ì‹œê°„ = ì‹¤í–‰ ì‹œê°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./Data/fur/final_fur_validation.txt',\n",
    "    block_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./Data/fur/final_fur_test.txt',\n",
    "    block_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 104.95450854301453\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()  # ì‹œì‘ ì‹œê°„ ì €ì¥\n",
    "\n",
    "torch.save(dataset1, './Data/fur/RevBERT_fur_train.pkl')\n",
    "torch.save(dataset2, './Data/fur/RevBERT_fur_validation.pkl')\n",
    "torch.save(dataset3, './Data/fur/RevBERT_fur_test.pkl')\n",
    "\n",
    "print(\"time :\", time.time() - start)  # í˜„ì¬ì‹œê° - ì‹œì‘ì‹œê°„ = ì‹¤í–‰ ì‹œê°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 77.47024011611938\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()  # ì‹œì‘ ì‹œê°„ ì €ì¥\n",
    "\n",
    "dataset1 = torch.load('./Data/fur/RevBERT_fur_train.pkl')\n",
    "dataset2 = torch.load('./Data/fur/RevBERT_fur_validation.pkl')\n",
    "dataset3 = torch.load('./Data/fur/RevBERT_fur_test.pkl')\n",
    "\n",
    "print(\"time :\", time.time() - start)  # í˜„ì¬ì‹œê° - ì‹œì‘ì‹œê°„ = ì‹¤í–‰ ì‹œê°„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vbowPIDCFuhF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  8294,  1983,  4168,  4046, 21964,  8110,  2346,  2381, 15067,\n",
      "        15681,  9007,  2458,   595,  4113, 26349,  9260,     3])}\n"
     ]
    }
   ],
   "source": [
    "for example in dataset3.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='RevBERT_fur_model_output(300)',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=50,\n",
    "    per_device_eval_batch_size=50,\n",
    "    load_best_model_at_end=True,\n",
    "#     save_steps=1000, # step ìˆ˜ë§ˆë‹¤ ëª¨ë¸ì„ ì €ì¥\n",
    "#     save_total_limit=2, # ë§ˆì§€ë§‰ ë‘ ëª¨ë¸ ë¹¼ê³  ê³¼ê±° ëª¨ë¸ì€ ì‚­ì œ\n",
    "#     logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator, # ë°¥ì„ ì–´ë–»ê²Œ ë– ë¨¹ì—¬ì¤„ì§€\n",
    "    train_dataset=dataset1, # ë°¥ì´ ë­”ì§€\n",
    "    eval_dataset=dataset2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµ ì‹œ, 1 epochì— 9ì‹œê°„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "mavrUbPahUF9",
    "outputId": "372c2de1-5d0b-446e-9565-5716591f4483",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1022772\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 50\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 102280\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102280' max='102280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102280/102280 26:50:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.306200</td>\n",
       "      <td>2.196736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.131300</td>\n",
       "      <td>2.028296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.042500</td>\n",
       "      <td>1.958388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.975300</td>\n",
       "      <td>1.904566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.905300</td>\n",
       "      <td>1.869639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.871300</td>\n",
       "      <td>1.846041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.821600</td>\n",
       "      <td>1.790462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.773800</td>\n",
       "      <td>1.749942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>1.715889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.734500</td>\n",
       "      <td>1.735123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-10228\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-10228\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-10228\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-20456\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-20456\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-20456\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-30684\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-30684\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-30684\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-40912\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-40912\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-40912\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-51140\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-51140\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-51140\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-61368\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-61368\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-61368\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-71596\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-71596\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-71596\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-81824\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-81824\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-81824\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-92052\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-92052\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-92052\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-102280\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-102280\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-102280\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from RevBERT_fur_model_output(300)\\checkpoint-92052 (score: 1.7158886194229126).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102280, training_loss=1.9682160232418628, metrics={'train_runtime': 96660.3699, 'train_samples_per_second': 105.811, 'train_steps_per_second': 1.058, 'total_flos': 1.009044764922096e+18, 'train_loss': 1.9682160232418628, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BERT pre-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
