{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G8TD17pT89K",
    "outputId": "ac9fa96e-0e70-46ee-995d-9d72d22fa193"
   },
   "outputs": [],
   "source": [
    "!mkdir wordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXQvXY7jUvMD",
    "outputId": "4fba4a49-f990-49c3-8e9d-3aea5ddad0b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MLqabAUgXPoF"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from filelock import FileLock\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformers.utils import logging\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer #bertformaskedLM\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "logger = logging.get_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a75xM9WsUwRi"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "HNMUWRoM5j6a",
    "outputId": "785fe9a6-11bd-4aa0-e78a-c10c9b58a30d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.20.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_review_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>할인 받아서 좋은 가격에 구매했어요 남편이 이것만 사용해서 자주구매 합니다 좋아요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>보통</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>아직 사용전이긴 하나 말 그대로 요긴하게 쓰일 듯하네요 어묵꽂이 할 때 필요해서 주...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>만족합니다 전 리터 시켰고 요 솔찍히 빨래가 저 같은 경우 많을 때 가 잇 옅 적을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>좋아요 탈착할 수 있어 더욱 좋아요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042767</th>\n",
       "      <td>일 구입 후 차례 기분 좋게 사용 했습니다 사용 후 조리대위에서 뚜껑 분리하여 세...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042768</th>\n",
       "      <td>수고하세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042769</th>\n",
       "      <td>저희 집은 원목식탁이라 아이들 식탁 매트로 사용하려고 구매했어요 사이즈가 측정 후 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042770</th>\n",
       "      <td>강블리 물맛이 좋네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042771</th>\n",
       "      <td>직배송 복사용지 용지 복사지 매 박스 더블에 이</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1042772 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               pre_review_\n",
       "0           할인 받아서 좋은 가격에 구매했어요 남편이 이것만 사용해서 자주구매 합니다 좋아요 \n",
       "1                                                       보통\n",
       "2        아직 사용전이긴 하나 말 그대로 요긴하게 쓰일 듯하네요 어묵꽂이 할 때 필요해서 주...\n",
       "3        만족합니다 전 리터 시켰고 요 솔찍히 빨래가 저 같은 경우 많을 때 가 잇 옅 적을...\n",
       "4                                      좋아요 탈착할 수 있어 더욱 좋아요\n",
       "...                                                    ...\n",
       "1042767   일 구입 후 차례 기분 좋게 사용 했습니다 사용 후 조리대위에서 뚜껑 분리하여 세...\n",
       "1042768                                              수고하세요\n",
       "1042769  저희 집은 원목식탁이라 아이들 식탁 매트로 사용하려고 구매했어요 사이즈가 측정 후 ...\n",
       "1042770                                       강블리 물맛이 좋네요 \n",
       "1042771                         직배송 복사용지 용지 복사지 매 박스 더블에 이\n",
       "\n",
       "[1042772 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_pickle('./Data/fur_train_data.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = data['pre_review_'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delab_7\\AppData\\Local\\Temp\\ipykernel_9368\\1359118793.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, line in tqdm_notebook(enumerate(doc[:-20000])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5644f62d532b4b03865a077ed3d98be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "with open('./Data/fur/final_fur_train.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, line in tqdm_notebook(enumerate(doc[:-20000])):\n",
    "#         for line in lines.split('. '):\n",
    "#             line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n",
    "#             line = re.sub(' +', ' ', line)\n",
    "        f.write(str(line) + '\\n')\n",
    "    if idx != len(doc[:-20000])-1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delab_7\\AppData\\Local\\Temp\\ipykernel_9368\\3341214965.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, line in tqdm_notebook(enumerate(doc[-20000:-10000])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2faa61c159384fe88f31f3d5d434cf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# validation\n",
    "with open('./Data/fur/final_fur_validation.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, line in tqdm_notebook(enumerate(doc[-20000:-10000])):\n",
    "#         for line in lines.split('. '):\n",
    "#             line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n",
    "#             line = re.sub(' +', ' ', line)\n",
    "        f.write(str(line) + '\\n')\n",
    "    if idx != len(doc[-20000:10000])-1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delab_7\\AppData\\Local\\Temp\\ipykernel_9368\\3402576644.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for idx, line in tqdm_notebook(enumerate(doc[-10000:])):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4a9fd8a1f54b6cba0a9263e2e619e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "with open('./Data/fur/final_fur_test.txt', 'w', encoding='utf-8') as f:\n",
    "    for idx, line in tqdm_notebook(enumerate(doc[-10000:])):\n",
    "#         for line in lines.split('. '):\n",
    "#             line = re.sub('[%s]' % re.escape(string.punctuation), '', line)\n",
    "#             line = re.sub(' +', ' ', line)\n",
    "        f.write(str(line) + '\\n')\n",
    "    if idx != len(doc[-10000:])-1:\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, BertForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108950064"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YSmzT_Y8d_yE"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids 0 = [PAD]\n",
      "Token ids 1 = [UNK]\n",
      "Token ids 2 = [CLS]\n",
      "Token ids 3 = [SEP]\n",
      "Token ids 4 = [MASK]\n"
     ]
    }
   ],
   "source": [
    "print(\"Token ids 0 = {}\".format(tokenizer.convert_ids_to_tokens(0)))\n",
    "print(\"Token ids 1 = {}\".format(tokenizer.convert_ids_to_tokens(1)))\n",
    "print(\"Token ids 2 = {}\".format(tokenizer.convert_ids_to_tokens(2)))\n",
    "print(\"Token ids 3 = {}\".format(tokenizer.convert_ids_to_tokens(3)))\n",
    "print(\"Token ids 4 = {}\".format(tokenizer.convert_ids_to_tokens(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at ./Data/fur/final_fur_train.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 76.78861427307129\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "dataset1 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./Data/fur/final_fur_train.txt',\n",
    "    block_size=300,\n",
    ")\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./Data/fur/final_fur_validation.txt',\n",
    "    block_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./Data/fur/final_fur_test.txt',\n",
    "    block_size=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 104.95450854301453\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "torch.save(dataset1, './Data/fur/RevBERT_fur_train.pkl')\n",
    "torch.save(dataset2, './Data/fur/RevBERT_fur_validation.pkl')\n",
    "torch.save(dataset3, './Data/fur/RevBERT_fur_test.pkl')\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 77.47024011611938\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "dataset1 = torch.load('./Data/fur/RevBERT_fur_train.pkl')\n",
    "dataset2 = torch.load('./Data/fur/RevBERT_fur_validation.pkl')\n",
    "dataset3 = torch.load('./Data/fur/RevBERT_fur_test.pkl')\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vbowPIDCFuhF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2,  8294,  1983,  4168,  4046, 21964,  8110,  2346,  2381, 15067,\n",
      "        15681,  9007,  2458,   595,  4113, 26349,  9260,     3])}\n"
     ]
    }
   ],
   "source": [
    "for example in dataset3.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='RevBERT_fur_model_output(300)',\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=50,\n",
    "    per_device_eval_batch_size=50,\n",
    "    load_best_model_at_end=True,\n",
    "#     save_steps=1000, # step 수마다 모델을 저장\n",
    "#     save_total_limit=2, # 마지막 두 모델 빼고 과거 모델은 삭제\n",
    "#     logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator, # 밥을 어떻게 떠먹여줄지\n",
    "    train_dataset=dataset1, # 밥이 뭔지\n",
    "    eval_dataset=dataset2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki 전체 데이터로 학습 시, 1 epoch에 9시간 정도 소요됩니다!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "mavrUbPahUF9",
    "outputId": "372c2de1-5d0b-446e-9565-5716591f4483",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1022772\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 50\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 100\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 102280\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102280' max='102280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102280/102280 26:50:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.306200</td>\n",
       "      <td>2.196736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.131300</td>\n",
       "      <td>2.028296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.042500</td>\n",
       "      <td>1.958388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.975300</td>\n",
       "      <td>1.904566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.905300</td>\n",
       "      <td>1.869639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.871300</td>\n",
       "      <td>1.846041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.821600</td>\n",
       "      <td>1.790462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.773800</td>\n",
       "      <td>1.749942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.745000</td>\n",
       "      <td>1.715889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.734500</td>\n",
       "      <td>1.735123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-10228\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-10228\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-10228\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-20456\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-20456\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-20456\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-30684\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-30684\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-30684\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-40912\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-40912\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-40912\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-51140\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-51140\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-51140\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-61368\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-61368\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-61368\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-71596\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-71596\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-71596\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-81824\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-81824\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-81824\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-92052\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-92052\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-92052\\pytorch_model.bin\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "D:\\Anaconda3\\envs\\review\\lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 100\n",
      "Saving model checkpoint to RevBERT_fur_model_output(300)\\checkpoint-102280\n",
      "Configuration saved in RevBERT_fur_model_output(300)\\checkpoint-102280\\config.json\n",
      "Model weights saved in RevBERT_fur_model_output(300)\\checkpoint-102280\\pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from RevBERT_fur_model_output(300)\\checkpoint-92052 (score: 1.7158886194229126).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102280, training_loss=1.9682160232418628, metrics={'train_runtime': 96660.3699, 'train_samples_per_second': 105.811, 'train_steps_per_second': 1.058, 'total_flos': 1.009044764922096e+18, 'train_loss': 1.9682160232418628, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "BERT pre-training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
